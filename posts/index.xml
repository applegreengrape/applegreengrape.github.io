<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on DevOps | Cloud | Golang | Python | API</title>
    <link>/posts/</link>
    <description>Recent content in Posts on DevOps | Cloud | Golang | Python | API</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Wed, 12 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Serverless Lambda Dynamodb API</title>
      <link>/posts/serverless-lambda-dynamodb-api/</link>
      <pubDate>Wed, 12 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/serverless-lambda-dynamodb-api/</guid>
      <description>This post is about how to terraform up a serverless, scalable and resilient lambda API. I am using the 2019nCov-api as an example. In addition, we will have a look at terraform&amp;rsquo;s built-in functions to make a more reusable modules for api gateway and lambdas.
前面几篇都是写dev的, 今天这篇来写云架构和terraform. 如何用AWS api gateway，lambda和dynamodb做一个轻巧，可扩展的适应性强的数据接口（p.s. serverless, scalable and resilient 这个我不知道该怎么翻， 反正就是这个☁️云架构精致，小巧且耐用)
Use Case 用例 The use case is I want to build a simple API to allow user to retrieve coronavirus data.</description>
    </item>
    
    <item>
      <title>Map Data Visualization</title>
      <link>/posts/data-map-visualization/</link>
      <pubDate>Sun, 09 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/data-map-visualization/</guid>
      <description>This post is about what I have learned about visualizing data from 2019nCov-api. In order to visualize the data I will need to get the latitude-longitude reference. And I found two interesting APIs. Baidu Map vs Google Map.
The coordinate systems is actually quite interesting. Baidu Maps uses a variant of web Mercator projection for slicing map data into tiles with an underlying latitude-longitude reference - BD-09 coordinate system while the Chinese government is using GCJ-02.</description>
    </item>
    
    <item>
      <title>COVID-19 API</title>
      <link>/posts/2019ncov-api/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/2019ncov-api/</guid>
      <description>The beginning of the year of rat is quite difficult for most Chinese people since the coronavirus outbreak. I have been checking the news about the progress of this outbreak. One day I watched the interview of a Chinese health official. She mentioned that there was one patient who has no idea that he has met anyone from Wuhan,the center of the outbreak. However, they used big data and found 3 people from Wuhan that have been in contact with him.</description>
    </item>
    
    <item>
      <title>Payment Gateway | Go | Stripe | Vault</title>
      <link>/posts/go-payment-gateway-with-stripe-and-vault/</link>
      <pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/go-payment-gateway-with-stripe-and-vault/</guid>
      <description>In my previous posts, I have talked about how to build your own Google NLP API. You are a smart developer and you got a great idea 💡. You build an amazing API with useful functions. The only thing you are missing is a validated business case. I think the eaiset way to prove the use case is to put it in the market and see whether or not people are willing to pay for it.</description>
    </item>
    
    <item>
      <title>Go NLP API | Build Your Own </title>
      <link>/posts/go-nlp-api/</link>
      <pubDate>Wed, 13 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/go-nlp-api/</guid>
      <description>In my previous posts, I have talked about building a small backend server to handle HTTP POST request. We also have tried out the Google NLP and we dived into the topic of NLTK. Well, now we know how to create a Go function to handle HTTP request and we understand the idea of Natural language Processing. Why not building your own Google NLP API?
Let&amp;rsquo;s now look at the key functions // Now we need to create a function to parse and tag all the data // There is a pre-built lib you can use - &amp;#34;gopkg.</description>
    </item>
    
    <item>
      <title>AWS Resources Visualizer | yUML | Boto3 | Python </title>
      <link>/posts/uml-visualize-your-cloud/</link>
      <pubDate>Sat, 19 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/uml-visualize-your-cloud/</guid>
      <description>This is an example about how to use aws boto3 with yUML to visualize your aws cloud resources. Please feel free to 👉📱message my twilio bot +447479275693.I will come back to you shortly 😃.
 In my last post, I have talked about how to play with aws boto3 api. During a usual dev-ish-ops day, sometimes I need to draw an aws network diagram. I have to log in to the console, click click click, and then copy-paste&amp;hellip;hmmmm&amp;hellip;It&amp;rsquo;s a little tiny bit boring and obviously, I want to automate it.</description>
    </item>
    
    <item>
      <title>Text Mining | NLTK | Python</title>
      <link>/posts/nltk-grammar-tag/</link>
      <pubDate>Tue, 08 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/nltk-grammar-tag/</guid>
      <description>In my previous posts, we have crawled some articles and news through RSS feed and HTML tags. We played with google&amp;rsquo;s nlp api and then as a developer without too much pocket money to play with. You end up looking for cheaper and better alternatives 👉nltk (natual language toolkit).
Let&amp;rsquo;s break the sentence down and let your code to define how will it read 🤓
How about let the code understand your emotions and 👉 you to the grammar mistake you made.</description>
    </item>
    
    <item>
      <title>Text Mining | Google NLP | Python</title>
      <link>/posts/google-nlp-python/</link>
      <pubDate>Mon, 07 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/google-nlp-python/</guid>
      <description>So I have complained about the Google&amp;rsquo;s Spreadsheets API. After I crawled the news, I feel like I should do some text mining to break the sentences down and teach my algo to read the news. And I came across Google&amp;rsquo;s NLP API, Ha! I know I won&amp;rsquo;t like it but why not?
1. let&amp;rsquo;s create a google-nlp class # Imports the Google Cloud client library from google.cloud import language from google.</description>
    </item>
    
    <item>
      <title>Web Scraper | Python</title>
      <link>/posts/web-scraper-python/</link>
      <pubDate>Mon, 09 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/web-scraper-python/</guid>
      <description>For the Golang version please check here.
And here is a python version:
&amp;#34;&amp;#34;&amp;#34;create a rss class to parse the rss feed url anf get the title and link &amp;#34;&amp;#34;&amp;#34; import feedparser import os class rss: def __init__(req, url): req.url = url def fetchDetails(req): try: url = req.url last_etag = &amp;#39;&amp;#39; last_modified = &amp;#39;&amp;#39; title = &amp;#39;&amp;#39; link = &amp;#39;&amp;#39; row = &amp;#39;&amp;#39; result = [] feed = feedparser.parse(url) last_etag = feed.</description>
    </item>
    
    <item>
      <title>Export GCP Stackdriver Log With Filebeat</title>
      <link>/posts/export-gcp-stackdriver-log-with-filebeat/</link>
      <pubDate>Thu, 05 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/export-gcp-stackdriver-log-with-filebeat/</guid>
      <description>This is a bash script to configure GCP project to export logs by creating a Pub/Sub sink topic and let filebeat to subscribe to that sink topic by the filebeat google cloud module.
#!/bin/sh # author: me 😃 # $ bash gcloud-admin.sh -h Required parameters: # -id|--project-id: gcloud project id # -svs|--svs-account: gcloud service account name to collect logs # Optional parameters: # -h|--help: Print this message readonly ARGS=&amp;#34;$@&amp;#34; readonly dependencies=( &amp;#34;gcloud&amp;#34; ) processArgs(){ while [[ &amp;#34;$#&amp;#34; -gt 0 ]]; do key=&amp;#34;$1&amp;#34; case &amp;#34;$key&amp;#34; in -h|--help) PRINT_HELP=true shift ;; -id|--project-id) PROJECT_ID=&amp;#34;$2&amp;#34; shift ;; -svs|--svs-account) SVS_ACCOUNT=&amp;#34;$2&amp;#34; shift ;; esac shift done } checkDependencies() { local unmet_dependencies=false for dependency in &amp;#34;${dependencies[@]}&amp;#34; ; do command -v &amp;#34;${dependency}&amp;#34; &amp;gt;/dev/null 2&amp;gt;&amp;amp;1 || { echo &amp;gt;&amp;amp;2 &amp;#34;${dependency}required&amp;#34;; unmet_dependencies=true } done if [ &amp;#34;${unmet_dependencies}&amp;#34; = true ] ; then echo &amp;#34;Please install unmet dependencies above before running.</description>
    </item>
    
    <item>
      <title>AWS | Boto3 | Python </title>
      <link>/posts/boto3-python/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/boto3-python/</guid>
      <description>This is an example about how to create your own python boto3 class and use it in your day-to-day work 😃. Please feel free to 👉📱message my twilio bot +447479275693. I will come back to you shortly 😃.
 import boto3 import os &amp;#34;&amp;#34;&amp;#34;how to use this classimport aws_modules.get_all_sg_rulessg_rule = aws_modules.get_all_sg_rules.sg(aws_account) # passing aws_account value to retrive all sg rulessg_rule_result = sg_rule.getSgRules()&amp;#34;&amp;#34;&amp;#34; class sg: def __init__(req, aws_account): req.aws_account = aws_account def getSgRules(req): try: os.</description>
    </item>
    
    <item>
      <title>Google Spreadsheets Paper Test By Calling Google&#39;s Spreadsheets API</title>
      <link>/posts/papertest-google-spreadsheet-api/</link>
      <pubDate>Tue, 13 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/papertest-google-spreadsheet-api/</guid>
      <description>I have made one post about doing paper test by using google app script. For a fully automated paper testing, you can use Google&amp;rsquo;s Spreadsheets API to track your paper test result. How to do it?
1. Connect to Google&amp;rsquo;s Spreadsheets API There is a quickstart documentation. And I have posted mine:
package main import ( &amp;#34;time&amp;#34; &amp;#34;encoding/json&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;io/ioutil&amp;#34; &amp;#34;log&amp;#34; &amp;#34;net/http&amp;#34; &amp;#34;os&amp;#34; &amp;#34;golang.org/x/net/context&amp;#34; &amp;#34;golang.org/x/oauth2&amp;#34; &amp;#34;golang.org/x/oauth2/google&amp;#34; &amp;#34;google.golang.org/api/sheets/v4&amp;#34; ) // Retrieve a token, saves the token, then returns the generated client.</description>
    </item>
    
    <item>
      <title>amazon s3</title>
      <link>/posts/aws-s3-command/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/aws-s3-command/</guid>
      <description>aws s3 cli is great! You can easily move your local files to your aws s3 buckets. However, sometimes it is not that easy to do simple tasks - like copy files where there is a whitespace in the file name, delete all versions of all files in a versioned s3 bucket and the difference between aws s3 sync and aws s3 cp --recursive .
escape the whitespace in your file name When you have a long list of files that you need to upload to s3 bucket, it will be easy for you to loop it through if you have nice filenames that there are no whitespace.</description>
    </item>
    
    <item>
      <title>Google Spreadsheets Paper Test</title>
      <link>/posts/papertest-google-app-script/</link>
      <pubDate>Mon, 12 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/papertest-google-app-script/</guid>
      <description>I was inspired by the alpaca&amp;rsquo;s idea of using google spreadsheet as a test environment for auto algo trading. So I decided to make a version for bitcoin paper test trading. For alpaca&amp;rsquo;s stock version, you can find the details here. The google spreadsheet will look like this:
I am using coinbase api to get the lastest Bitcoin price. And there is already one google app script class that you can use - Class UrlFetchApp.</description>
    </item>
    
    <item>
      <title>jq examples</title>
      <link>/posts/useful-jq-commands/</link>
      <pubDate>Sun, 11 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/useful-jq-commands/</guid>
      <description>jq is a command-line JSON processor to parse json format data. You can find the detailed documentation here. And you can try to play it online at jqplay.org. I am listing out few jq command line examples that I found quite useful for day-to-day work 😃.
Let&amp;rsquo;s try jq with AWS resources api.
// to parse and extract json data $ curl -s \ https://pricing.us-east-1.amazonaws.com/offers/v1.0/aws/index.json \ | jq .offers.AmazonEC2 { &amp;#34;offerCode&amp;#34;: &amp;#34;AmazonEC2&amp;#34;, &amp;#34;versionIndexUrl&amp;#34;: &amp;#34;/offers/v1.</description>
    </item>
    
    <item>
      <title>Twilio Message Bot 📟</title>
      <link>/posts/twilio-message-bot/</link>
      <pubDate>Fri, 12 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/twilio-message-bot/</guid>
      <description>So you want a personal chatbot to handle your messages. Let&amp;rsquo;s build a easy twilio bot to handle and filter your incoming messages and calls - your personal virtual assistant 📟. Let&amp;rsquo;s start with a simple user case. I want to have a 📟 to recieve the SMS messages and send me a summary about who are contacting me, what messages have them sent and what are their contact numbers.</description>
    </item>
    
    <item>
      <title>Lambda Logshipper</title>
      <link>/posts/lambda-logshipper/</link>
      <pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/lambda-logshipper/</guid>
      <description>How can you easily move your Cloudwatch logstream to another platform or log collector endpoints? The easiest way is to ship the Cloudwatch logstream through a socket client. This is an example of a small Golang lambda function to ship aws cloudwatch log stream to a tcp endpoint.
you will need a socket client:
func SocketClient(m []byte) { conn, err := net.Dial(&amp;#34;tcp&amp;#34;, &amp;#34;your_tcp_endpoint:your_port&amp;#34;) defer conn.Close() if err != nil { fmt.</description>
    </item>
    
    <item>
      <title>Web Scraper | Golang</title>
      <link>/posts/web-scraper/</link>
      <pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>/posts/web-scraper/</guid>
      <description>So you are interested in news and events and you want to track the latest news and headlines. And I guess you are familiar with the concept of RSS Rich Site Summary. Okay,😃Let&amp;rsquo;s start to build a simple Golang application to fetch the latest news and headlines with in a second.
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;github.com/mmcdole/gofeed&amp;#34; ) func feed(){ fp := gofeed.NewParser() feed, err := fp.ParseURL(&amp;#34;http://feeds.reuters.com/reuters/UKTopNews&amp;#34;) if err != nil { panic(err) } for _, item := range feed.</description>
    </item>
    
  </channel>
</rss>